

## 机器学习公平性综述

**标题：A Review on Fairness in Machine Learning**

文章的结构：

1. 引言部分主要使用了大量例子说明了公平性很重要且受关注。

2. 分析了潜在的不公平性原因
3. 如何度量公平性
4. 增加公平性的机制
5. 关于算法公平性的新兴研究



### 公平性相关案例

> Since many automated decisions (including which individuals will receive jobs, loans, medication, bail, or parole) can significantly impact people’s lives, there is great importance in assessing and improving the ethics of the decisions made by these automated systems.Indeed, in recent years, the concern for algorithm fairness has made headlines.

文章首先陈述了AI 算法公平性的重要性，以及当前受到的关注，这段表述值得模仿，接下来作者开始举例，给出了三个例子：

- > U.S. criminal justice system had falsely predicted future criminality among African-Americans at twice the rate as it predicted for white people.
- > Amazon discovered that their ML hiring system was discriminating against female candidates, particularly for software development and technical positions.
- > Google’s ad-targeting algorithm had proposed higher-paying executive jobs more for men than for women.

三个例子各是「美国犯罪评估系统」、「亚马逊雇佣系统」、「谷歌广告投放系统」；三个例子可以留着以后写作的时候化用；以上三个例子说明了算法的不公平现象是普遍存在的，继而人们对此展开了相关的研究工作——

> These lines of evidence and concerns about algorithmic fairness have led to growing interest in the literature on defining, evaluating, and improving fairness in ML algorithms. It is important to note, however, that the task of improving fairness of ML algorithms is **NOT trivial** since there exists an inherent **TRADE-OFF between accuracy and fairness**. In other words, as we pursue a higher degree of fairness, we may compromise accuracy

准确率与公平性之间的权衡，是由 *Inherent trade-offs in the fair determination of risk scores* 这篇文章指出来的。然而提高模型公平性是一件有代价的事情，模型的准确率与公平性是此消彼长的关系，这一点其实非常类似于分类模型的准确性与对抗鲁棒性。本文主要讨论的也是分类模型——

> Our survey also attempts to **cover the pros and cons** of the various measures and mechanisms, and guide under which setting they should be used. and guide under which setting they should be used. Finally, although the main part of this article deals primarily with classification tasks, a major goal of this survey is **to highlight and discuss emerging areas of research**, beyond classification, that are expected to grow in the upcoming years. 



### 不公平性的潜在原因

> We start this section by noting that throughout this article, we use the terms bias, discrimination, and unfairness interchangeably with similar meanings, as is commonly done in the algorithmic fairness literature.

首先文章告诉我们不加区分的使用的 bias、discrimination、 unfairness 三个单词，然后给我们列出来一系列潜在的可能导致不公平的原因，主要包括数据与算法两个方面，数据又可分成三个方面，**数据本身包含偏见**、**由于采样等原因造成数据缺失引起偏见**，以及 **有可能促使模型隐式推出偏见的属性**，我们重点讨论数据方面，

- > Biases already included in the datasets used for learning, which are based on biased device measurements, historically biased human decisions, erroneous reports, or other reasons. ML algorithms are essentially designed to replicate these biases. 

- > Biases caused by missing data, such as missing values or sample/selection biases, which result in datasets that are not representative of the target population

- > Biases caused by **proxy attributes** for sensitive attributes. Sensitive attributes differentiate privileged and unprivileged groups, **such as race, gender, and age,** and are typically not legitimate for use in decision making. Proxy attributes are non-sensitive attributes that can be exploited to derive sensitive attributes. The ML algorithm can implicitly make decisions based on the sensitive attributes under the cover of using presumably legitimate attributes

其实代理属性 (proxy attribute) 这个角度蛮有意思，比如说，种族有可能导致偏见，所以特征收集的时候没有考虑种族，但是不同种族居住的地方是有地理规律的，于是邮编号码这个属性便有可能导致种族偏见。

> Algorithmic objectives aim at minimizing overall aggregated prediction errors and therefore benefit majority groups over minorities

至于算法方面，与数据偏见第一条是差不多的，因为算法要最小化损失函数，自然是照顾多数样本并忽略少数样本；关于这里，我曾经见过一个说法——激活函数会使得模型学到的特征呈现马太分布——马太分布 (Matthew Effect) 是指在某个领域中，越是优秀的人越容易获得更多的资源和机会，而越是弱势的人则越容易失去资源和机会。马太分布在深度学习中有时也被用来描述神经元之间的差异化现象。一般来说，使用饱和激活函数（e.g. Sigmoid或Tanh）可能会导致部分神经元输出接近0或1，从而造成梯度消失或爆炸，使得模型难以更新权重，如此一来，导致个别的神经元比其他神经元更有效地参与训练和预测，继而形成马太分布。

这个环节作者给出一组定义：优势群体 (privileged)、劣势群体 (unprivileged)，然后使用一个案例说明了选人制度的不平等，比如美国高考 SAT 分数，若以这个指标选人极有可能出现偏见，因为缺乏教育资源的同学如果考出 1100 分数，那其实际水平与那些占有优质教育资源的同学但是考出 1400 分数的同学，具有相似的潜力。这个案例如果本土化——江苏考生高考来考北京卷，简直嘎嘎乱杀。

从这个角度来说，由于国内一些领域过度内卷，大量人才投递简历，加剧了企业的筛选成本，迫使用人单位不得不以学历筛人，从而忽视了一些真正有潜力的从业者，招入了一些高分低能的无潜力者。如果能够建立一套高效、公平的个人潜力评估系统，或许能够打破国内学历内卷的现状。这一点或许能够作为创新点。



### 公平性如何度量

>  The legal domain has introduced two main definitions of discrimination: 
>
> - (i) **disparate treatment**: intentionally treating an individual differently based on his/her membership in a protected class (direct discrimination);
> - (ii) **disparate impact**: negatively affecting members of a protected class more than others even if by a seemingly neutral policy (indirect discrimination).
>
> Put in our context, it is important to note that algorithms trained with data that do not include sensitive attributes (i.e., attributes that explicitly identify the protected and unprotected groups) are unlikely to produce disparate treatment but may still induce unintentional discrimination in the form of disparate impact

如果能够确保数据不包含敏感属性，那么算法能够避免显式歧视，但是隐式歧视仍然无法完全消除，我们希望不同群体在分类上面具有相同的可能性，文章使用 odds 这个词来描述事情发生的可能——

- Disparate impart $\cfrac{P[\hat{Y}=1|S\neq 1]}{P[\hat{Y}=1|S = 1]}\leq 1 - \epsilon$
- Demographic parity $|{P[\hat{Y}=1|S = 1]}-{P[\hat{Y}=1|S \neq 1]}|\leq \epsilon$

> One disadvantage of these two measures is that a fully accurate classifier may be considered unfair, when the base rates (i.e., the proportion of actual positive outcomes) of the various groups are significantly different.

然后如果不同群体人口基础差异较大的时候，一个完全正确的模型会被认为是有偏见的，为此引入了下面这种度量方式——度量不同群体在某个事件上面的发生的可能性是否均等

- Equalized odds:
  $$
  |{P[\hat{Y}=1|S = 1,Y=0]}-{P[\hat{Y}=1|S \neq 1,Y=0]}|\leq \epsilon\\
  |{P[\hat{Y}=1|S = 1,Y=1]}-{P[\hat{Y}=1|S \neq 1,Y=1]}|\leq \epsilon
  $$

- Equal opportunity:
  $$
  |{P[\hat{Y}=1|S = 1,Y=1]}-{P[\hat{Y}=1|S \neq 1,Y=1]}|\leq \epsilon
  $$



> This measure considers other individual attributes for defining fairness rather than just the sensitive attributes. However, note that to define similarity between individuals, a similarity metric needs to be defined, which is not trivial. This measure, in addition to assuming a similarity metric, also requires some assumptions regarding the relationship between features and labels

以上度量都是针对群体而言，也有一些研究引入针对个体的度量方式 (Individual fairness)，但是这种方式需要假设一种度量方法可靠，这个步骤呀也是有代价的。
$$
|{P[\hat{Y}^{(i)}=y|X^{(i)},S^{(i)}]}-{P[\hat{Y}^{(j)}=y|X^{(j)},S^{(j)}]}|\leq \epsilon, \text{ if } d(i,j)\approx 0
$$
然而以上的这些指标多少都有一点问题，例如敏感属性如何确定，以及如果敏感属性并非二值数值怎么办？如果敏感属性是一个有限的定类变量，似乎可以使用 one-hot 编码的方式转为多个二值变量，但若敏感属性是一个数值型变量呢？





### 公平性如何增强

> Numerous recent papers have proposed mechanisms to enhance fairness in ML algorithms. These mechanisms are typically categorized into three types: pre-process, in-process, and post-process.

关于如何增加公平性的机制，主要围绕想到了事前，事中，时候三个不同阶段开展，其中事前的主要是对数据进行操作，事中主要是对算法进行操作，

- Pre-Process Mechanisms

  - 重新采样：通过增加或减少某些群体的样本数量来平衡数据集；

  - 重新加权：通过给某些群体的样本分配不同的权重来影响模型的损失函数；

  - 重构特征：通过删除、转换或生成新特征来减少或消除敏感属性相关信息，这个方法类似于对隐私数据的进行数据混淆或删除，会涉及降维方法；

  - 生成合成数据：通过使用生成模型，生成新数据来补充数据集的多样性；

- In-Process Mechanism (主要是指如何训练一个模型)

  > Zafar et al. [214, 215] and Woodworth et al. [209] suggest adding constraints to the classification model that require satisfying a proxy for equalized odds [209, 214]ordisparate impact [215]. Woodworth et al. [209] also show that there exist difficult computational challenges in learning a fair classifier based on equalized odds.

  - 事中处理的主要是添加添加组件，或者正则项等等，209, 214, 215 三篇文章把上面提到的几个公平性度量指标用到了目标函数上面，因而关于如何添加正则项这一点，以上三篇可以继续细看——
    - **[209]** Learning non-discriminatory predictors.
    - **[214]** Fairness beyond disparate treatment and disparate impact: Learning classification without disparate mistreatment
    - **[215]** Fairness constraints: Mechanisms for fair classification.

- Post-Process Mechanisms (如何使用一个训练好的模型)

  > Corbett-Davies et al. and Menon and Williamson similarly suggest selecting separate thresholds for each group separately, in a manner that maximizes accuracy and minimizes demographic parity. Dwork et al. propose a decoupling technique to learn a different classifier for each group.

  - 为每个群体单独设置一个激活阈值
  - 为每个群体单独设置一个分类器

有一个研究方向称为 **差分隐私**，这个方向**事前处理**似乎有着些许关联。我觉得重点可以看看**事中处理**的部分，事前处理、事后处理的部分简单粗暴，尤其是事后处理，有一点不考虑成本，但是好处在于事前、事后的处理手段几乎适用于任何算法。





### 关于算法公平性的新兴研究

本节介绍了一些公平性在机器学习中除了分类之外的其他应用领域，包括时序数据分析、推荐系统、聚类、回归、自然语言处理和计算机视觉。这些领域都有各自的公平性定义、度量和方法，也有各自的挑战和未来的研究方向。这一节主要概述了这些子领域的最新进展，并且提供一些参考文献供感兴趣的读者进一步阅读。

- Fair Sequential Learning

- Fair Word Embedding

- Fair Visual Description

- Fair Recommender Systems

- Fair Private Learning

- Fair Adversarial Learning

  > To use GANs for fair learning, previous studies have developed different approaches. Models that are based on GANs are often constructed as minimax optimization problems that aim at maximizing the predictor’s capability to accurately predict the outcomes while minimizing the adversary’s capability to predict the sensitive feature.

- Fair Causal Learning

  > Causal approaches may assist in enhancing fairness in several manners. For instance, by understanding causes and effects in the data, the model may assist in tackling the challenges of fairness definitions by analyzing which types of discrimination should be allowed and which should not.

我觉得 Fair Adversarial Learning 这个环节说不定有点搞头，比如现在的 AI 绘画其实可以用来生成大量现实中不存在的图像样本来丰富数据库。



## 在线选择中的公平与偏见

**标题：Fairness and Bias in Online Selection**